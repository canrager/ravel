{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zLdMswKcLn1"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_DIR = f'/share/u/can/ravel'\n",
    "SRC_DIR = os.path.join(REPO_DIR, 'src')\n",
    "MODEL_DIR = os.path.join(REPO_DIR, 'models')\n",
    "DATA_DIR = os.path.join(REPO_DIR, 'data')\n",
    "\n",
    "for d in [MODEL_DIR, DATA_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "import sys\n",
    "for d in [REPO_DIR, SRC_DIR]:\n",
    "    sys.path.append(d)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9-AQQvZdOJV"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "751b10379fa04369a928de0c169bc8a5",
      "ee54d0bb220a4fc990300851def006b9",
      "17fa097292c14f368b378455bdb34f00",
      "d8fd7c065e0f40a6a072b9674e151a9b",
      "d7e1ff0cf538459c929ab11f9a085f36",
      "4d0a842f1abd429ea7760a616e543dec",
      "cebd2e681d3348c9854cf39f54cf630c",
      "81a8f701fbf94221bd43b193154cb97e",
      "585aaa1f9d0644e49c672924c4972724",
      "5b620de26aa241a2a1a0013f1e18780f",
      "f7d2396c43894a0c8599dd2c7537d993",
      "70c1b6ad24154b23a2a093b30e90e381",
      "405ac3d1406746e294b95c7bb439da4c",
      "74b005f710cd4e00a3c95ddd67adafce",
      "912af663642347adb8aad11f8a64d549",
      "932170e034ea4cfab7f1441ea918e1d1",
      "b2fda2b2096d40baa193dd4d1a19d87a",
      "52831983547d495bb6fe49ebb529651f",
      "121a8a8060004ec78528b4a6610d258c",
      "299491b58aec4a9a833d5eebb663f7dc",
      "759bd9c482e443f6bbe37061ade5b90e",
      "7bd0541c2f554cf8a928478e91155a2e",
      "810760e38dfd4a93bd3d8d6f403fd8ca",
      "8a44a29537574aa4a646ee4ec2576349",
      "09da734f5e504bca8a1acadabcb5587e",
      "5575f534c7824383a7605a6410a3179b",
      "fed41c03de5b46ef8823018c9b073fb8",
      "14899a8d47f344eb891c7c9a6c8be1f7",
      "cd57e162f7b74446a0513cf506033efe",
      "643f4b03189244ec9ee82058544c6466",
      "1c02c5bc227e434a9cfd61dedbaaa8f6",
      "a128a521389241b0a652ce881c82ffbf",
      "9650c36433ad4834ac0c2e837bdceee1",
      "f37fc22440e84f7daa780c6a2e1aa4ed",
      "ef05646e03e44bbcae8897070c37f715",
      "ddb0fa3f8b7d49f883111688bff55c23",
      "c261774d123c4fd48d410f32c6eebc56",
      "e4c2aba9c08b4fe79a1ba09c3737ddd5",
      "4d30dbf65c7644e6b54de02622f73c78",
      "161946e826514151be288093133e4687",
      "3522ec77b10549519492b2460ea0c067",
      "749a258238d74c8db1c11e835248af30",
      "8b84a2e90dd44a999dc88314222a1202",
      "911948b078154c6a8188760b21782e88",
      "c5ab60d821bd49aaad49220515a978d9",
      "4c429b9b4958486199caae8968f653e1",
      "ac7d96ccf7044106a71e2b78f1203adf",
      "db60dda6866c450dab11f3ecc59c220a",
      "8d40065e7a3b43acaa5d21d8d758e933",
      "eb4cfccc2087486da19ca406e49b3e36",
      "edd02c77ab6e4ccba8306ee65ba90bfb",
      "fd1effd5b7064aca88acd7964d42f980",
      "4a81929a53764f91a7f817dd373bd626",
      "82c060949fbf4cb6932969be027994ea",
      "6c75fbb2a7f743e4bbbb3cd8438e9924",
      "dfe3c2d31f404ff3b3327c17e7c0e380",
      "93bf1b72097248dfa0b3c25a3047447a",
      "245645b8c8f548e2ab70d069fac4c8e2",
      "1a5142f6a25e4b919bf6ecbf77ecd16d",
      "4c333e5bda86444c8cb3bf5f0ab0fd0c",
      "bfc5b7b37a2e4624ad2936f4d33030cd",
      "189ade82b95c4398b8b0edae04294ae5",
      "d2f05dcf69ef401c82199b6492d6e804",
      "02250c1f0df44fe181c36e1a910fde42",
      "6b06ca6b71f34b4eb9f5545dba30d34e",
      "bbaf8442e0524728b4e10edf905985e3",
      "0633455563354788ae5efdd3ee49e48b",
      "49184e4498e8438496db8cb2b33d3cbf",
      "78f4a73ff33f4dee93791bfafd7b0146",
      "ab7a96d0a5f249fe86b64df0e42d0c55",
      "5be2896cb3cc482eb6ab1a0c506f00a6",
      "1c422a05945b44c6a1b12b4a03b46800",
      "b5c9054f8c644877a133d5448216df41",
      "24fe9cd28993448eac55ec8c66cbf0b5",
      "6f5170b160e04e179da978bfdf9dad64",
      "21368d4272824b3e9387701b8c325e0c",
      "6456e65205b749a89f9e721bf145a7ba"
     ]
    },
    "id": "IrrLMNHoqAiF",
    "outputId": "dc3ae720-7a6f-4cfc-affb-a13877e7e85d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "model_name = \"tinyllama\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_DIR)\n",
    "hf_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, device_map='auto', cache_dir=MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16)\n",
    "hf_model = hf_model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight\n",
    "nnsight_tracer_kwargs = {'scan': False, 'validate': False, 'use_cache': False, 'output_attentions': False}\n",
    "\n",
    "nnsight_model = NNsight(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from nnsight import NNsight\n",
    "\n",
    "# torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "# with open('/share/u/can/src/hf.txt', 'r') as f:\n",
    "#     hf_token = f.read().strip()\n",
    "\n",
    "# model_id = \"google/gemma-2-2b\"\n",
    "# model_name = \"gemma-2-2b\"\n",
    "\n",
    "# hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     cache_dir=MODEL_DIR,\n",
    "#     token=hf_token,\n",
    "#     device_map=device,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     attn_implementation=\"eager\"\n",
    "# )\n",
    "\n",
    "# nnsight_model = NNsight(model)\n",
    "\n",
    "# tokenizer =  AutoTokenizer.from_pretrained(\n",
    "#     \"google/gemma-2-2b\",\n",
    "#     cache_dir=MODEL_DIR,\n",
    "#     token=hf_token,\n",
    "# )\n",
    "\n",
    "# VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkttf-519P79"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 14\n",
    "entity_type = 'city'\n",
    "INPUT_MAX_LEN = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jPOMM9W0-BL",
    "outputId": "da9e40e7-3813-4a8e-80f2-ac7d046b8e1c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "# Load training dataset.\n",
    "split_to_raw_example = json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_train.json'), 'r'))\n",
    "# Load validation + test dataset.\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_context_test.json'), 'r')))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{model_name}/{model_name}_{entity_type}_entity_test.json'), 'r')))\n",
    "# Prepend an extra token to avoid tokenization changes for Llama tokenizer.\n",
    "# Each sequence will start with <s> _ 0\n",
    "SOS_PAD = '0'\n",
    "NUM_SOS_TOKENS = 3\n",
    "for split in split_to_raw_example:\n",
    "  for i in range(len(split_to_raw_example[split])):\n",
    "    split_to_raw_example[split][i]['inv_label'] = SOS_PAD + split_to_raw_example[split][i]['inv_label']\n",
    "    split_to_raw_example[split][i]['label'] = SOS_PAD + split_to_raw_example[split][i]['label']\n",
    "\n",
    "\n",
    "# Load attributes (tasks) to prompt mapping.\n",
    "ALL_ATTR_TO_PROMPTS = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "# Load prompt to intervention location mapping.\n",
    "split_to_entity_pos = json.load(open(os.path.join(DATA_DIR, model_name, f'{model_name}_{entity_type}_prompt_to_entity_position.json')))\n",
    "SPLIT_TO_INV_LOCATIONS = {\n",
    "    f'{task}{split}': {'max_input_length': INPUT_MAX_LEN,\n",
    "                       'inv_position': [INPUT_MAX_LEN + pos]}\n",
    "    for task, pos in split_to_entity_pos.items()\n",
    "    for split in ('-train', '-test', '-val', '')\n",
    "}\n",
    "assert(min([min(v['inv_position']) for v in SPLIT_TO_INV_LOCATIONS.values()]) > 0)\n",
    "\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def filter_inv_example(example):\n",
    "  return (example['label'] != example['inv_label'] and\n",
    "          example['source_split'] in SPLIT_TO_INV_LOCATIONS and\n",
    "          example['split'] in SPLIT_TO_INV_LOCATIONS)\n",
    "\n",
    "for split in split_to_raw_example:\n",
    "  random.shuffle(split_to_raw_example[split])\n",
    "  split_to_raw_example[split] = list(filter(filter_inv_example, split_to_raw_example[split]))\n",
    "  if len(split_to_raw_example[split]) == 0:\n",
    "    print('Empty split: \"%s\"' % split)\n",
    "# Remove empty splits.\n",
    "split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "print(f\"#Training examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-train')]))}, \"\n",
    "      f\"#Validation examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-val')]))}, \"\n",
    "      f\"#Test examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-test')]))}\")\n",
    "split_to_dataset = {split: Dataset.from_list(\n",
    "    split_to_raw_example[split], features=FEATURE_TYPES)\n",
    "                    for split in split_to_raw_example}\n",
    "\n",
    "# #Training examples=116728, #Validation examples=20516, #Test examples=22497"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKpdF24zzJN-"
   },
   "source": [
    "# Sparse Autoencoder (SAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZuIUk2DZaO"
   },
   "source": [
    "## Training\n",
    "\n",
    "We will train a sparse autoencoder on entity representations extracted offline.\n",
    "\n",
    "* Download entity representations extracted from the Wikipedia dataset [here](https://drive.google.com/file/d/1hZ-Nv3ehf0Ok4ic3ybe-DATEh-HRjYkt/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzRVRB6DDZYe",
    "outputId": "80197f5d-6504-4551-c2e7-d2004bffdaa8"
   },
   "outputs": [],
   "source": [
    "# from scripts.train_sae import train_sae\n",
    "# import re\n",
    "\n",
    "# config = {\n",
    "#     'task_name': task_name,\n",
    "#     'reg_coeff': float(re.search('reg([\\d.]+)', task_name).group(1)),\n",
    "#     'input_dim': model.config.hidden_size,\n",
    "#     'latent_dim': int(re.search('dim(\\d+)', task_name).group(1)),\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'weight_decay': 1e-4,\n",
    "#     'end_learning_rate_ratio': 0.5,\n",
    "#     'num_epochs': int(re.search('ep(\\d+)', task_name).group(1)),\n",
    "#     'model_dir': MODEL_DIR,\n",
    "#     'log_dir': os.path.join(MODEL_DIR, 'logs', task_name),\n",
    "# }\n",
    "\n",
    "# # Training metrics are logged to the Tensorboard at http://localhost:6006/.\n",
    "# # autoencoder = train_sae(config, wiki_train_dataloader, wiki_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6E0eudgO2F"
   },
   "outputs": [],
   "source": [
    "autoencoder_run_name = 'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k.pt'\n",
    "autoencoder = torch.load(os.path.join(MODEL_DIR, autoencoder_run_name)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JumpRelu SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sae_repo_id = \"google/gemma-scope-2b-pt-res\"\n",
    "sae_filename = \"layer_20/width_16k/average_l0_71/params.npz\"\n",
    "autoencoder_run_name = (sae_repo_id + '-' + sae_filename.replace('/', '-')).replace('.npz', '')\n",
    "\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    "    cache_dir= os.path.join(MODEL_DIR, model_name),\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class JumpReluAutoEncoder(torch.nn.Module):\n",
    "  \"\"\"Sparse Autoencoder with a two-layer encoder and a two-layer decoder.\"\"\"\n",
    "\n",
    "  def __init__(self, embed_dim, latent_dim, device):\n",
    "    super().__init__()\n",
    "    self.dtype = torch.float32\n",
    "    self.embed_dim = embed_dim\n",
    "    self.latent_dim = latent_dim\n",
    "    self.W_enc = nn.Parameter(torch.empty(embed_dim, latent_dim))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(latent_dim))\n",
    "    self.W_dec = nn.Parameter(torch.empty(latent_dim, embed_dim))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(embed_dim))\n",
    "    self.threshold = nn.Parameter(torch.zeros(latent_dim))\n",
    "    self.autoencoder_losses = {}\n",
    "\n",
    "  def encode(self, x, normalize_input=False):\n",
    "    if normalize_input:\n",
    "      raise ValueError(\"Not supported\")\n",
    "      x = x - self.decoder[0].bias\n",
    "    pre_jump = x @ self.W_enc + self.b_enc\n",
    "\n",
    "    f = nn.ReLU()(pre_jump * (pre_jump > self.threshold))\n",
    "    # Decoder weights are not normalized. Thus we have to compensate here to get comparabe feature activations.\n",
    "    f = f * self.W_dec.norm(dim=1)\n",
    "    return f\n",
    "\n",
    "  def decode(self, z):\n",
    "    # Decoder weights are not normalized. Thus we have to compensate here to get comparabe feature activations.\n",
    "    z = z / self.W_dec.norm(dim=1)\n",
    "    return z @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, base):\n",
    "    base_type = base.dtype\n",
    "    base = base.to(self.dtype)\n",
    "    self.autoencoder_losses.clear()\n",
    "    z = self.encode(base)\n",
    "    base_reconstruct = self.decode(z)\n",
    "    # The sparsity objective.\n",
    "    l1_loss = torch.nn.functional.l1_loss(z, torch.zeros_like(z))\n",
    "    # The reconstruction objective.\n",
    "    l2_loss = torch.mean((base_reconstruct - base)**2)\n",
    "    self.autoencoder_losses['l1_loss'] = l1_loss\n",
    "    self.autoencoder_losses['l2_loss'] = l2_loss\n",
    "    return {'latent': z, 'output': base_reconstruct.to(base_type)}\n",
    "\n",
    "  def get_autoencoder_losses(self):\n",
    "    return self.autoencoder_losses\n",
    "\n",
    "  def from_pretrained(\n",
    "          path: str | None = None, \n",
    "          load_from_sae_lens: bool = False,\n",
    "          device: torch.device | None = None,\n",
    "          **kwargs,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Load a pretrained autoencoder from a file.\n",
    "    If sae_lens=True, then pass **kwargs to sae_lens's\n",
    "    loading function.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path)\n",
    "    latent_dim, embed_dim = state_dict['W_enc'].shape\n",
    "    autoencoder = JumpReluAutoEncoder(embed_dim, latent_dim)\n",
    "    autoencoder.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = params['W_enc'].shape[0]\n",
    "latent_dim = params['W_enc'].shape[1]\n",
    "\n",
    "sae = JumpReluAutoEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    device=device,\n",
    ")\n",
    "sae.load_state_dict(pt_params)\n",
    "sae.to(device)\n",
    "\n",
    "autoencoder = sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBClAbyY0Tm0"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp data/tinyllama/ravel_city_tinyllama_layer14_representation.hdf5 data/ravel_city_tinyllama_layer14_representation.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OeMovewbVdp",
    "outputId": "f4897d1b-43b3-4fe4-c103-0ea1e57b1296"
   },
   "outputs": [],
   "source": [
    "# Load the RAVEL dataset.\n",
    "import json\n",
    "\n",
    "from src.utils.dataset_utils import load_entity_representation_with_label\n",
    "\n",
    "splits = ['train', 'val_entity', 'val_context']\n",
    "feature_hdf5_path = os.path.join(DATA_DIR, f'ravel_{entity_type}_{model_name}_layer{layer_idx}_representation.hdf5')\n",
    "entity_attr_to_label = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_entity_attributes.json')))\n",
    "X, Y, sorted_unique_label = load_entity_representation_with_label(feature_hdf5_path, entity_attr_to_label, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPF6FR8zvwvJ"
   },
   "outputs": [],
   "source": [
    "# Run feature selection.\n",
    "import numpy as np\n",
    "\n",
    "from src.methods.select_features import select_features_with_classifier\n",
    "\n",
    "intervention_dim_to_eval = [\n",
    "    ('reconstruction', None),\n",
    "    ('dim%d' % autoencoder.latent_dim, range(autoencoder.latent_dim))]\n",
    "\n",
    "attr = 'Country'\n",
    "coeff_to_kept_dims = select_features_with_classifier(\n",
    "    autoencoder.encode, torch.from_numpy(X[attr]['train']).to(device), Y[attr]['train'])\n",
    "for kept_dim in coeff_to_kept_dims.values():\n",
    "  intervention_dim_to_eval.append(('dim%d' % len(kept_dim), kept_dim))\n",
    "# Random baselines.\n",
    "for i in [64, 512]:\n",
    "    kept_dim = np.random.permutation(autoencoder.latent_dim)[:i]\n",
    "    intervention_dim_to_eval.append(('random_dim%d' % len(kept_dim), kept_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O tinyllama.tgz \"https://huggingface.co/datasets/adamkarvonen/ravel/resolve/main/tinyllama.tgz?download=true\"\n",
    "# !tar -xzf tinyllama.tgz -C data/\n",
    "# !mkdir data/base\n",
    "# !tar -xvf data.tgz -C data/base --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0npE3xlExIz"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERY IMPORTANT NOTE\n",
    "In the below cell, we only use the first 10 elements of the dataset to speed up iteration. This should get increased once we have everything working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run eval\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "from src.utils.intervention_utils import load_intervenable_with_autoencoder, eval_with_interventions, remove_all_forward_hooks\n",
    "from src.utils.metric_utils import compute_metrics\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')\n",
    "                         }\n",
    "print(len(eval_split_to_dataset))\n",
    "\n",
    "# Keep only the first 10 items\n",
    "eval_split_to_dataset = dict(list(eval_split_to_dataset.items())[:10])\n",
    "print(f\"New length: {len(eval_split_to_dataset)}\")\n",
    "\n",
    "print(len(eval_split_to_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default pyvene implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyvene_intervention(\n",
    "    intervenable,\n",
    "    split_to_inv_locations,\n",
    "    inputs,\n",
    "    b_s,\n",
    "    num_inv,\n",
    "    max_new_tokens,\n",
    "    forward_only=False,\n",
    "):\n",
    "    intervention_locations = {\n",
    "        \"sources->base\": (\n",
    "            [\n",
    "                [\n",
    "                    split_to_inv_locations[inputs[\"source_split\"][i]][\"inv_position\"]\n",
    "                    for i in range(b_s)\n",
    "                ]\n",
    "            ]\n",
    "            * num_inv,\n",
    "            [\n",
    "                [\n",
    "                    split_to_inv_locations[inputs[\"split\"][i]][\"inv_position\"]\n",
    "                    for i in range(b_s)\n",
    "                ]\n",
    "            ]\n",
    "            * num_inv,\n",
    "        )\n",
    "    }\n",
    "    if not forward_only:\n",
    "        base_outputs, counterfactual_out_tokens = intervenable.generate(\n",
    "            {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            },\n",
    "            [\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"source_input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"source_attention_mask\"],\n",
    "                    \"position_ids\": inputs[\"source_position_ids\"],\n",
    "                }\n",
    "            ],\n",
    "            intervention_locations,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            intervene_on_prompt=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            output_original_output=True,\n",
    "        )\n",
    "    # else: # This seems deprecated in the demo notebook\n",
    "    #     base_outputs, counterfactual_outputs = intervenable(\n",
    "    #         {\n",
    "    #             \"input_ids\": inputs[\"input_ids\"],\n",
    "    #             \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    #             \"position_ids\": inputs[\"position_ids\"],\n",
    "    #         },\n",
    "    #         [\n",
    "    #             {\n",
    "    #                 \"input_ids\": inputs[\"source_input_ids\"],\n",
    "    #                 \"attention_mask\": inputs[\"source_attention_mask\"],\n",
    "    #                 \"position_ids\": inputs[\"source_position_ids\"],\n",
    "    #             }\n",
    "    #         ],\n",
    "    #         intervention_locations,\n",
    "    #         output_original_output=True,\n",
    "    #     )\n",
    "    #     counterfactual_logits = counterfactual_outputs.logits\n",
    "    #     counterfactual_out_tokens = torch.argmax(counterfactual_outputs.logits, dim=-1)\n",
    "    #     base_outputs = torch.argmax(base_outputs.logits, dim=-1)\n",
    "\n",
    "    return base_outputs, counterfactual_out_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nnsight replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "tracer_kwargs = {'scan': False, 'validate': False, 'use_cache': False, 'output_attentions': False}\n",
    "\n",
    "\n",
    "def nnsight_intervention(\n",
    "    nnsight_model,\n",
    "    layer,\n",
    "    autoencoder,\n",
    "    inv_dims,\n",
    "    inputs,\n",
    "    split_to_inv_locations,\n",
    "    n_generated_tokens,\n",
    "    add_reconstruction_error=True,\n",
    "):  \n",
    "    batch_size = inputs['input_ids'].shape[0]\n",
    "    submodule = nnsight_model.model.layers[layer]\n",
    "\n",
    "    # Organize sequence positions for interventions\n",
    "    base_inv_positions, source_inv_positions = [], []\n",
    "    for i in range(batch_size):\n",
    "        base_inv_positions.append(\n",
    "            split_to_inv_locations[inputs[\"split\"][i]][\"inv_position\"]\n",
    "        )\n",
    "        source_inv_positions.append(\n",
    "            split_to_inv_locations[inputs[\"source_split\"][i]][\"inv_position\"]\n",
    "        )\n",
    "    base_inv_positions = torch.tensor(base_inv_positions, device=device)\n",
    "    source_inv_positions = torch.tensor(source_inv_positions, device=device)\n",
    "    \n",
    "    # Indexing preparation\n",
    "    if isinstance(inv_dims, range):\n",
    "        inv_dims = torch.tensor(list(inv_dims), device=device)\n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    batch_arange = einops.repeat(batch_arange, 'b -> b d', d=inv_dims.shape[0])\n",
    "    source_inv_positions = einops.repeat(source_inv_positions.squeeze(), 'b -> b d', d=inv_dims.shape[0])\n",
    "    inv_dims = einops.repeat(inv_dims, 'd -> b d', b=batch_size)\n",
    "    \n",
    "    # Forward pass on source input\n",
    "    with torch.no_grad(), nnsight_model.trace(\n",
    "        inputs['source_input_ids'],\n",
    "        attention_mask=inputs['source_attention_mask'],\n",
    "        position_ids=inputs['source_position_ids'],\n",
    "        **tracer_kwargs,\n",
    "    ):\n",
    "        llm_acts = submodule.output[0] #resid activations are on index 0\n",
    "        source_sae_acts = autoencoder.encode(llm_acts)\n",
    "        source_sae_acts = source_sae_acts[batch_arange, source_inv_positions, inv_dims].save() # Select only the intervention positions\n",
    "\n",
    "    # Forward pass on base input with intervention\n",
    "    for i in range(n_generated_tokens):\n",
    "        with torch.no_grad(), nnsight_model.trace(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            position_ids=inputs['position_ids'], \n",
    "            **tracer_kwargs):\n",
    "            llm_acts = submodule.output[0] # resid activations are on index 0\n",
    "\n",
    "            base_sae_acts = autoencoder.encode(llm_acts)\n",
    "            llm_acts_reconstructed = autoencoder.decode(base_sae_acts)\n",
    "            reconstruction_error = llm_acts - llm_acts_reconstructed\n",
    "\n",
    "            base_sae_acts[batch_arange, base_inv_positions, inv_dims] = source_sae_acts # Replace the intervention positions with the source intervention positions\n",
    "            llm_acts_intervened = autoencoder.decode(base_sae_acts)\n",
    "            if add_reconstruction_error:\n",
    "                corrected_acts = llm_acts_intervened + reconstruction_error\n",
    "            else:\n",
    "                corrected_acts = llm_acts_intervened\n",
    "            submodule.output = (corrected_acts.to(torch.bfloat16),)\n",
    "            counterfactual_logits = nnsight_model.lm_head.output.save()\n",
    "\n",
    "        # Append generation\n",
    "        next_token_logits = counterfactual_logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'], next_token], dim=-1)\n",
    "        inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.ones(batch_size, 1, device=device)], dim=-1)\n",
    "        inputs['position_ids'] = torch.cat([inputs['position_ids'], (inputs['position_ids'][:, -1] +1).unsqueeze(-1) ], dim=-1)\n",
    "\n",
    "    return inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "def nnsight_intervention(nnsight_model, layer, autoencoder, inv_dims, inputs, split_to_inv_locations, n_generated_tokens, device, add_reconstruction_error=True):\n",
    "    batch_size = inputs['input_ids'].shape[0]\n",
    "    submodule = nnsight_model.model.layers[layer]\n",
    "    \n",
    "    # Organize intervention positions\n",
    "    base_inv_positions = torch.tensor([split_to_inv_locations[inputs[\"split\"][i]][\"inv_position\"] for i in range(batch_size)], device=device)\n",
    "    source_inv_positions = torch.tensor([split_to_inv_locations[inputs[\"source_split\"][i]][\"inv_position\"] for i in range(batch_size)], device=device)\n",
    "    \n",
    "    # Indexing preparation\n",
    "    inv_dims = torch.tensor(list(inv_dims), device=device) if isinstance(inv_dims, range) else inv_dims\n",
    "    batch_arange = einops.repeat(torch.arange(batch_size, device=device), 'b -> b d', d=inv_dims.shape[0])\n",
    "    source_inv_positions = einops.repeat(source_inv_positions.squeeze(), 'b -> b d', d=inv_dims.shape[0])\n",
    "    inv_dims = einops.repeat(inv_dims, 'd -> b d', b=batch_size)\n",
    "    \n",
    "    # Forward pass on source input\n",
    "    with torch.no_grad(), nnsight_model.trace(inputs['source_input_ids'], attention_mask=inputs['source_attention_mask'], position_ids=inputs['source_position_ids'], **nnsight_tracer_kwargs):\n",
    "        source_sae_acts = autoencoder.encode(submodule.output[0])\n",
    "        source_sae_acts = source_sae_acts[batch_arange, source_inv_positions, inv_dims].save()\n",
    "\n",
    "    # Forward pass on base input with intervention\n",
    "    for _ in range(n_generated_tokens):\n",
    "        with torch.no_grad(), nnsight_model.trace(inputs['input_ids'], attention_mask=inputs['attention_mask'], position_ids=inputs['position_ids'], **nnsight_tracer_kwargs):\n",
    "            llm_acts = submodule.output[0]\n",
    "            base_sae_acts = autoencoder.encode(llm_acts)\n",
    "            llm_acts_reconstructed = autoencoder.decode(base_sae_acts)\n",
    "\n",
    "            base_sae_acts[batch_arange, base_inv_positions, inv_dims] = source_sae_acts\n",
    "\n",
    "            if not add_reconstruction_error:\n",
    "                submodule.output = (corrected_acts.to(torch.bfloat16),)\n",
    "            else:\n",
    "                reconstruction_error = llm_acts - llm_acts_reconstructed\n",
    "                corrected_acts = llm_acts_reconstructed + reconstruction_error\n",
    "                submodule.output = (corrected_acts.to(torch.bfloat16),)\n",
    "\n",
    "            counterfactual_logits = nnsight_model.lm_head.output.save()\n",
    "\n",
    "        # Append generation\n",
    "        next_token = torch.argmax(counterfactual_logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'], next_token], dim=-1)\n",
    "        inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.ones(batch_size, 1, device=device)], dim=-1)\n",
    "        inputs['position_ids'] = torch.cat([inputs['position_ids'], (inputs['position_ids'][:, -1] + 1).unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    return inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.intervention_utils import (\n",
    "    is_llama_tokenizer,\n",
    "    get_dataloader,\n",
    "    remove_invalid_token_id,\n",
    ")\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def eval_with_interventions(\n",
    "    hf_model, # Native Hugging Face model\n",
    "    intervenable, # Pyvene model wrapper\n",
    "    nnsight_model,  # NNsight model wrapper\n",
    "    split_to_dataset,\n",
    "    split_to_inv_locations,\n",
    "    tokenizer,\n",
    "    compute_metrics_fn,\n",
    "    max_new_tokens=1,\n",
    "    eval_batch_size=16,\n",
    "    debug_print=False,\n",
    "    forward_only=False,\n",
    "    use_nnsight_replication=False,\n",
    "):\n",
    "    split_to_eval_metrics = {}\n",
    "    padding_offset = 3 if is_llama_tokenizer(tokenizer) else 0\n",
    "    num_inv = len(intervenable.interventions)\n",
    "    for split in tqdm(split_to_dataset):\n",
    "        # Asssume all inputs have the same max length.\n",
    "        prompt_max_length = split_to_inv_locations[split_to_dataset[split][0][\"split\"]][\n",
    "            \"max_input_length\"\n",
    "        ]\n",
    "        eval_dataloader = get_dataloader(\n",
    "            split_to_dataset[split],\n",
    "            tokenizer=tokenizer,\n",
    "            batch_size=eval_batch_size,\n",
    "            prompt_max_length=prompt_max_length,\n",
    "            output_max_length=padding_offset + max_new_tokens,\n",
    "            first_n=max_new_tokens,\n",
    "        )\n",
    "        eval_labels = collections.defaultdict(list)\n",
    "        eval_preds = []\n",
    "        with torch.no_grad():\n",
    "            if debug_print:\n",
    "                epoch_iterator = tqdm(eval_dataloader, desc=f\"Test\")\n",
    "            else:\n",
    "                epoch_iterator = eval_dataloader\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "                position_ids = {\n",
    "                    f\"{prefix}position_ids\": intervenable.model.prepare_inputs_for_generation(\n",
    "                        input_ids=inputs[f\"{prefix}input_ids\"],\n",
    "                        attention_mask=inputs[f\"{prefix}attention_mask\"],\n",
    "                    )[\"position_ids\"]\n",
    "                    for prefix in (\"\", \"source_\")\n",
    "                }\n",
    "                inputs.update(position_ids)\n",
    "                for key in inputs:\n",
    "                    if key in (\n",
    "                        \"input_ids\",\n",
    "                        \"source_input_ids\",\n",
    "                        \"attention_mask\",\n",
    "                        \"source_attention_mask\",\n",
    "                        \"position_ids\",\n",
    "                        \"source_position_ids\",\n",
    "                        \"labels\",\n",
    "                        \"base_labels\",\n",
    "                    ):\n",
    "                        inputs[key] = inputs[key].to(intervenable.model.device)\n",
    "\n",
    "                if not use_nnsight_replication:\n",
    "                    base_outputs, counterfactual_out_tokens = pyvene_intervention(\n",
    "                        intervenable,\n",
    "                        split_to_inv_locations,\n",
    "                        inputs,\n",
    "                        b_s,\n",
    "                        num_inv,\n",
    "                        max_new_tokens,\n",
    "                        forward_only=forward_only,\n",
    "                    )\n",
    "                    eval_preds.append(counterfactual_out_tokens)\n",
    "                else:\n",
    "                    base_outputs = hf_model.generate(\n",
    "                        inputs[\"input_ids\"],\n",
    "                        attention_mask=inputs[\"attention_mask\"],\n",
    "                        max_length=inputs[\"input_ids\"].shape[1] + max_new_tokens,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        num_beams=1,\n",
    "                        do_sample=False,\n",
    "                        output_scores=True,\n",
    "                    )\n",
    "                    counterfactual_out_tokens = nnsight_intervention(\n",
    "                        nnsight_model,\n",
    "                        layer_idx,\n",
    "                        autoencoder,\n",
    "                        inv_dims,\n",
    "                        inputs,\n",
    "                        split_to_inv_locations,\n",
    "                        n_generated_tokens=max_new_tokens,\n",
    "                    )\n",
    "                    eval_preds.append(counterfactual_out_tokens)\n",
    "                    \n",
    "                for label_type in [\"base_labels\", \"labels\"]:\n",
    "                    eval_labels[label_type].append(inputs[label_type])\n",
    "                eval_labels[\"base_outputs\"].append(base_outputs[:, -max_new_tokens:])\n",
    "                if debug_print and step < 3:\n",
    "                    print(\"\\nInputs:\")\n",
    "                    print(\"Base:\", inputs[\"input\"][:3])\n",
    "                    print(\"Source:\", inputs[\"source_input\"][:3])\n",
    "                    print(\"Tokens to intervene:\")\n",
    "                    print(\n",
    "                        \"Base:\",\n",
    "                        tokenizer.batch_decode(\n",
    "                            [\n",
    "                                inputs[\"input_ids\"][i][\n",
    "                                    intervention_locations[\"sources->base\"][1][0][i]\n",
    "                                ]\n",
    "                                for i in range(len(inputs[\"split\"]))\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Source:\",\n",
    "                        tokenizer.batch_decode(\n",
    "                            [\n",
    "                                inputs[\"source_input_ids\"][i][\n",
    "                                    intervention_locations[\"sources->base\"][0][0][i]\n",
    "                                ]\n",
    "                                for i in range(len(inputs[\"split\"]))\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                    base_output_text = tokenizer.batch_decode(\n",
    "                        base_outputs[:, -max_new_tokens:], skip_special_tokens=True\n",
    "                    )\n",
    "                    print(\"Base Output:\", base_output_text)\n",
    "                    print(\n",
    "                        \"Output:    \",\n",
    "                        tokenizer.batch_decode(counterfactual_out_tokens[:, -max_new_tokens:]),\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Inv Label: \",\n",
    "                        tokenizer.batch_decode(\n",
    "                            remove_invalid_token_id(\n",
    "                                inputs[\"labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                            )\n",
    "                        ),\n",
    "                    )\n",
    "                    base_label_text = tokenizer.batch_decode(\n",
    "                        remove_invalid_token_id(\n",
    "                            inputs[\"base_labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                        ),\n",
    "                        skip_special_tokens=True,\n",
    "                    )\n",
    "                    print(\"Base Label:\", base_label_text)\n",
    "                    if base_label_text != base_output_text:\n",
    "                        print(\"WARNING: Base outputs does not match base labels!\")\n",
    "        eval_metrics = {\n",
    "            label_type: compute_metrics_fn(\n",
    "                eval_preds,\n",
    "                eval_labels[label_type],\n",
    "                last_n_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                extra_labels=eval_labels,\n",
    "                eval_label_type=label_type,\n",
    "            )\n",
    "            for label_type in eval_labels\n",
    "            if label_type.endswith(\"labels\")\n",
    "        }\n",
    "        print(\"\\n\", repr(split) + \":\", eval_metrics)\n",
    "        split_to_eval_metrics[split] = {\n",
    "            \"metrics\": eval_metrics,\n",
    "            \"inv_outputs\": tokenizer.batch_decode(counterfactual_out_tokens[:, -max_new_tokens:]),\n",
    "            \"inv_labels\": tokenizer.batch_decode(\n",
    "                remove_invalid_token_id(\n",
    "                    inputs[\"labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                )\n",
    "            ),\n",
    "            \"base_labels\": tokenizer.batch_decode(\n",
    "                remove_invalid_token_id(\n",
    "                    inputs[\"base_labels\"][:, :max_new_tokens], tokenizer.pad_token_id\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "    return split_to_eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_nnsight_replication = True\n",
    "\n",
    "# Run eval\n",
    "import re\n",
    "\n",
    "from src.utils.intervention_utils import load_intervenable_with_autoencoder, remove_all_forward_hooks\n",
    "from src.utils.metric_utils import compute_metrics\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')\n",
    "                         }\n",
    "\n",
    "target_task = 'Country'\n",
    "max_new_tokens = 3\n",
    "layer = int(re.search(r'layer(\\d+)', autoencoder_run_name).group(1))\n",
    "print(f'Layer={layer}')\n",
    "\n",
    "for inv_name, inv_dims in intervention_dim_to_eval:\n",
    "  if inv_name == 'reconstruction':\n",
    "    continue\n",
    "  intervenable = load_intervenable_with_autoencoder(hf_model, autoencoder, inv_dims, layer)\n",
    "  intervenable.set_device(\"cuda\")\n",
    "  intervenable.disable_model_gradients()\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "    hf_model=hf_model,\n",
    "    intervenable=intervenable,\n",
    "    nnsight_model=nnsight_model,\n",
    "    split_to_dataset=eval_split_to_dataset,\n",
    "    split_to_inv_locations=SPLIT_TO_INV_LOCATIONS,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    compute_metrics_fn=compute_metrics,\n",
    "    eval_batch_size=128,\n",
    "    debug_print=False,\n",
    "    use_nnsight_replication=use_nnsight_replication,\n",
    "  )\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{autoencoder_run_name.split(\".pt\")[0]}_{inv_name}_{max_new_tokens}tok_{target_task}.json'), 'w'))\n",
    "  remove_all_forward_hooks(intervenable)\n",
    "  del intervenable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuMUHhoBohzd"
   },
   "source": [
    "# Distributed Alignment Search (DAS/MDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onC3-T2h0ulg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "from src.methods.distributed_alignment_search import LowRankRotatedSpaceIntervention\n",
    "from src.methods.differential_binary_masking import DifferentialBinaryMasking\n",
    "import pyvene as pv\n",
    "from tqdm import tqdm, trange\n",
    "from scripts.train_intervention import train_intervention\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from src.utils.dataset_utils import get_multitask_dataloader\n",
    "from src.utils.intervention_utils import train_intervention_step, eval_with_interventions, get_intervention_config, remove_all_forward_hooks, remove_invalid_token_id\n",
    "from src.utils.metric_utils import compute_metrics, compute_cross_entropy_loss\n",
    "\n",
    "\n",
    "def get_short_model_name(model):\n",
    "  name_match = re.search('(llama-2-\\d+b|tinyllama|pythia-[\\d.]+b)', model.name_or_path.lower())\n",
    "  if name_match:\n",
    "    return name_match.group(1)\n",
    "  else:\n",
    "    return model.name_or_path.lower().split('-')[0]\n",
    "\n",
    "\n",
    "def run_exp(config):\n",
    "  inv_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_source' in l])\n",
    "  control_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_base' in l])\n",
    "  task_compressed = ((inv_tasks + '_ex_' + control_tasks) if control_tasks else inv_tasks)\n",
    "  method_name = 'multitask_method' if len(config['training_tasks']) > 1 else 'baseline_method'\n",
    "  if config['intervenable_config']['intervenable_interventions_type'] == LowRankRotatedSpaceIntervention:\n",
    "    method_name = method_name.replace('method', 'daslora')\n",
    "  elif config['intervenable_config']['intervenable_interventions_type'] == DifferentialBinaryMasking:\n",
    "    if config['regularization_coefficient'] > 1e-6:\n",
    "      method_name = method_name.replace('method', 'mask_l1')\n",
    "    else:\n",
    "      method_name = method_name.replace('method', 'mask')\n",
    "  split_to_inv_locations = config['split_to_inv_locations']\n",
    "  input_len = list(split_to_inv_locations.values())[0]['max_input_length']\n",
    "  inv_pos = min([x['inv_position'][0] for x in split_to_inv_locations.values()])\n",
    "  inv_loc_name = 'len%d_pos%s' % (input_len, 'e' if inv_pos != input_len - 1 else 'f')\n",
    "  training_data_percentage = int(config['max_train_percentage'] * 100)\n",
    "  suffix = f\"_cause{config['cause_task_sample_size']}\"\n",
    "  if any([v == 'match_base' for t, v in config['training_tasks'].items()]):\n",
    "    suffix += f'_iso{config[\"iso_task_sample_size\"]}'\n",
    "  layer = config['intervenable_config']['intervenable_layer']\n",
    "  run_name = (f\"{get_short_model_name(model)}-layer{layer}\"\n",
    "              f\"-dim{config['intervention_dimension']}\"\n",
    "              f\"-{method_name}_{config['max_output_tokens']}tok_\"\n",
    "              f\"{task_compressed}_{inv_loc_name}_ep{config['training_epoch']}{suffix}\")\n",
    "  config['run_name_prefix'] = run_name.rsplit('_ep', 1)[0]\n",
    "  print(run_name)\n",
    "  intervenable, intervenable_config = train_intervention(config, model, tokenizer, split_to_dataset)\n",
    "  # Save model.\n",
    "  torch.save({k: v[0].rotate_layer.weight for k, v in intervenable.interventions.items()},\n",
    "             os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  print('Model saved to %s' % os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  # Eval.\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "      intervenable, eval_split_to_dataset, split_to_inv_locations, tokenizer,\n",
    "      compute_metrics_fn=compute_metrics,\n",
    "      max_new_tokens=config['max_output_tokens'],\n",
    "      eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  # Logging.\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))\n",
    "  print('Saved to %s' % os.path.join(MODEL_DIR, f'{run_name}_evalall.json'))\n",
    "  remove_all_forward_hooks(intervenable)\n",
    "  return intervenable\n",
    "\n",
    "\n",
    "attrs = list(ALL_ATTR_TO_PROMPTS)\n",
    "target_attr = 'Country'\n",
    "\n",
    "# Train on disentangling Country attribute only.\n",
    "training_tasks_list = [\n",
    "  {t: 'match_source'} for t in attrs if t == target_attr\n",
    "] + [\n",
    "    {t: 'match_source' if t == target_t else 'match_base' for t in attrs}\n",
    "    for target_t in attrs if target_t == target_attr\n",
    "]\n",
    "\n",
    "# eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "#                          if k.endswith('-test') or k.endswith('-val')}\n",
    "print(len(training_tasks_list))\n",
    "print(training_tasks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DGCvKEQxYsU"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = model.eval()\n",
    "\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 128\n",
    "\n",
    "lr = 1e-4\n",
    "for inv_layer in [14]:\n",
    "  for inv_dim in [64]:\n",
    "    for training_tasks in training_tasks_list:\n",
    "      for cause_task_sample_size in [20000]:\n",
    "        config = {\n",
    "            'regularization_coefficient': 0,\n",
    "            'intervention_dimension': inv_dim,\n",
    "            'max_output_tokens': 3,\n",
    "            'intervenable_config': {\n",
    "              'intervenable_layer': inv_layer,\n",
    "              'intervenable_representation_type': 'block_output',\n",
    "              'intervenable_unit': 'pos',\n",
    "              'max_number_of_units': 1,\n",
    "              'intervenable_interventions_type': LowRankRotatedSpaceIntervention,\n",
    "            },\n",
    "            'training_tasks': training_tasks,\n",
    "            'training_epoch': 3,\n",
    "            'split_to_inv_locations': SPLIT_TO_INV_LOCATIONS,\n",
    "            'max_train_percentage': 1.0 if len(training_tasks) <= 3 else 1.0,\n",
    "            'init_lr': lr,\n",
    "            'cause_task_sample_size': cause_task_sample_size,\n",
    "            'iso_task_sample_size': 4000,\n",
    "            'training_batch_size': TRAINING_BATCH_SIZE,\n",
    "            'task_to_prompts': ALL_ATTR_TO_PROMPTS,\n",
    "            'log_dir': os.path.join(MODEL_DIR, 'logs'),\n",
    "        }\n",
    "        intervenable = run_exp(config)\n",
    "\n",
    "\n",
    "# Training each method will take about 3.5 hrs on the hosted T4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glatG86QlJ4q"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgTg14xMlJCO"
   },
   "outputs": [],
   "source": [
    "# # The training script above has already included the evaluation part.\n",
    "# # Below is a standalone evaluation script in case you want to rerun evaluation.\n",
    "\n",
    "\n",
    "# import re\n",
    "\n",
    "# import pyvene as pv\n",
    "# from src.utils.intervention_utils import load_intervenable, load_intervenable_with_pca, eval_with_interventions\n",
    "# from src.utils.metric_utils import compute_metrics\n",
    "\n",
    "\n",
    "# model_paths = [\n",
    "#     'tinyllama-layer14-dim64-multitask_daslora_3tok_Country_ex_Continent+Latitude+Longitude+Language+Timezone_len48_pose_ep3_cause20000_iso4000.pt',\n",
    "#     'tinyllama-layer14-dim64-baseline_daslora_3tok_Country_len48_pose_ep3_cause20000.pt',\n",
    "#  ]\n",
    "\n",
    "# eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "#                          if k.endswith('-test')\n",
    "#                          }\n",
    "# RUN_TO_EVAL_METRICS = {}\n",
    "# for i, run_name in enumerate(model_paths):\n",
    "#   print(run_name)\n",
    "#   layer = int(re.search('layer(\\d+)[_\\-]', run_name).group(1))\n",
    "#   run_name, ext = run_name.rsplit('.', 1)\n",
    "#   if 'pca' in run_name:\n",
    "#     intervenable = load_intervenable_with_pca(model, run_name + '.' + ext)\n",
    "#   elif 'causal_abstraction' in run_name:\n",
    "#     # NOTE: This is not available\n",
    "#     intervenable = load_causal_abstraction_intervenable(model, run_name)\n",
    "#   else:\n",
    "#     intervenable = load_intervenable(model, os.path.join(MODEL_DIR, run_name + '.' + ext))\n",
    "#   split_to_eval_metrics = eval_with_interventions(\n",
    "#       intervenable, eval_split_to_dataset, SPLIT_TO_INV_LOCATIONS if layer < 24 else SPLIT_TO_INV_LOCATIONS_LAST_TOK,\n",
    "#       tokenizer, compute_metrics_fn=compute_metrics, max_new_tokens=3, debug_print=False)\n",
    "#   json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-_gzLeA1Qc7"
   },
   "source": [
    "# Compare Methods with Disentangle Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxZSB2bT36ra"
   },
   "outputs": [],
   "source": [
    "# Compute disentangle scores.\n",
    "\n",
    "from src.utils.metric_utils import compute_disentangle_score, compute_disentangle_scores_possible_empties\n",
    "\n",
    "\n",
    "tinyllama_dimension_to_log_path = {\n",
    "    'SAE': {d: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_dim{d}_3tok_Country.json'\n",
    "            # Update the following dimensions to match your own results.\n",
    "            # SAE might have different feature dimensions from run to run due to\n",
    "            # randomness in the feature selection algorithm.\n",
    "            for d in [71, 325, 399, 536, 8192]\n",
    "    },\n",
    "    # 'DAS': {d: f'tinyllama-layer14-dim{d}-baseline_daslora_3tok_Country_len48_pose_ep3_cause20000_evalall.json'\n",
    "    #         for d in [16, 64]},\n",
    "    # 'MDAS': {d: f'tinyllama-layer14-dim{d}-multitask_daslora_3tok_Country1_ex_Continent+Latitude+Longitude+Language+Timezone_len48_pose_ep3_cause20000_iso4000_evalall.json'\n",
    "    #          for d in [16, 64]\n",
    "    # },\n",
    "}\n",
    "# tinyllama_dimension_to_log_path['RandomSAE'] = {\n",
    "#             64: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_random_dim64_3tok_Country.json',\n",
    "#             512: f'tinyllama-layer14-dim8192-reg0.5-ep5-sae-city_wikipedia_200k_random_dim512_3tok_Country.json'\n",
    "#             }\n",
    "\n",
    "entity_type = 'city'\n",
    "target_attribute = 'Country'\n",
    "split_type = 'context'\n",
    "split_suffix = '-test'\n",
    "model_name = 'tinyllama'\n",
    "\n",
    "\n",
    "split_to_raw_example = json.load(\n",
    "    open(os.path.join(DATA_DIR, model_name, f'{model_name}_{entity_type}_{split_type}_test.json')))\n",
    "attribute_to_prompts = json.load(\n",
    "    open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "\n",
    "attribute_to_iso_tasks = {\n",
    "    a: [p + split_suffix for p in ps if p + split_suffix in split_to_raw_example]\n",
    "    for a, ps in attribute_to_prompts.items() if a != target_attribute}\n",
    "attribute_to_cause_tasks = {\n",
    "    a: [p + split_suffix for p in ps if p + split_suffix in split_to_raw_example]\n",
    "    for a, ps in attribute_to_prompts.items() if a == target_attribute}\n",
    "\n",
    "print(attribute_to_iso_tasks)\n",
    "\n",
    "for key in attribute_to_iso_tasks:\n",
    "  print(key)\n",
    "\n",
    "for key in attribute_to_cause_tasks:\n",
    "  print(key, \"F\")\n",
    "\n",
    "\n",
    "method_to_data = collections.defaultdict(dict)\n",
    "for method in tinyllama_dimension_to_log_path:\n",
    "  for inv_dimension in tinyllama_dimension_to_log_path[method]:\n",
    "    log_data = json.load(\n",
    "        open(os.path.join(MODEL_DIR, tinyllama_dimension_to_log_path[method][inv_dimension])))\n",
    "\n",
    "    # print(log_data)\n",
    "\n",
    "    # for key in log_data:\n",
    "    #   print(key, log_data[key], \"\\n\")\n",
    "\n",
    "    method_to_data[method][inv_dimension] = compute_disentangle_scores_possible_empties(\n",
    "        log_data, attribute_to_iso_tasks, attribute_to_cause_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming method_to_data is defined elsewhere in your code\n",
    "# method_to_data = defaultdict(<class 'dict'>, {'SAE': {71: {'disentangle': 0.41708333333333336, 'isolate': 0.49666666666666665, 'cause': 0.3375}, 325: {'disentangle': 0.4729166666666667, 'isolate': 0.06333333333333334, 'cause': 0.8825000000000001}, 399: {'disentangle': 0.4741666666666667, 'isolate': 0.05333333333333334, 'cause': 0.895}, 536: {'disentangle': 0.4779166666666667, 'isolate': 0.06333333333333334, 'cause': 0.8925000000000001}, 8192: {'disentangle': 0.47833333333333333, 'isolate': 0.06666666666666667, 'cause': 0.89}}})\n",
    "\n",
    "# Extract data for SAE method\n",
    "sae_data = method_to_data['SAE']\n",
    "\n",
    "# Extract x and y values for each metric\n",
    "x = list(sae_data.keys())\n",
    "metrics = ['disentangle', 'isolate', 'cause']\n",
    "y_values = {metric: [sae_data[key][metric] for key in x] for metric in metrics}\n",
    "\n",
    "# Create a single plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each metric\n",
    "for metric, values in y_values.items():\n",
    "    plt.plot(x, values, marker='o', label=metric)\n",
    "\n",
    "# Set plot attributes\n",
    "plt.title('SAE Metrics')\n",
    "plt.xlabel('Key')\n",
    "plt.ylabel('Score')\n",
    "plt.xscale('log')  # Set x-axis to logarithmic scale\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0GSxcY1esIh"
   },
   "outputs": [],
   "source": [
    "# #@markdown Plotting\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 100\n",
    "# plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "\n",
    "# colors = [matplotlib.colors.to_hex(c) for c in plt.cm.tab20.colors]\n",
    "\n",
    "# name_to_color = {\n",
    "#     'SAE_RAND': 'gray',\n",
    "#     'PCA': colors[6],\n",
    "#     'SAE': colors[2],\n",
    "#     'RLAP': colors[4],\n",
    "#     'DBM': colors[1],\n",
    "#     'MDBM': colors[0],\n",
    "#     'DAS': colors[9],\n",
    "#     'MDAS': colors[8],\n",
    "# }\n",
    "\n",
    "# name_to_marker = {\n",
    "#     'SAE_RAND': 'o--',\n",
    "#     'PCA': 'o--',\n",
    "#     'SAE': 'o--',\n",
    "#     'RLAP': '^--',\n",
    "#     'DBM': 's--',\n",
    "#     'MDBM': 's--',\n",
    "#     'DAS': 's--',\n",
    "#     'MDAS': 's--',\n",
    "# }\n",
    "\n",
    "# for n, x in method_to_data.items():\n",
    "#   sorted_dim = sorted(x, key=lambda i: float(i[:-1]))\n",
    "#   p = plt.plot([x[k][2] for k in sorted_dim],\n",
    "#                [x[k][1] for k in sorted_dim], name_to_marker[n], label=n, markersize=10,\n",
    "#                c=name_to_color[n])\n",
    "#   for k in sorted(x, key=lambda s: x[s][0], reverse=True):\n",
    "#     c = p[-1].get_color()\n",
    "#     offset = (0, 0.05)\n",
    "#     # Shift text boxes to avoid overlaps.\n",
    "#     if n == 'SAE' and k == '3.8%':\n",
    "#       offset = (0.05, -0.07)\n",
    "#     plt.annotate(k, (x[k][2] - offset[0], x[k][1] + offset[1]), size=12,\n",
    "#                  bbox=dict(boxstyle='round,pad=0.15', fc=c, ec='white', alpha=0.5))\n",
    "# plt.scatter(1, 1, s=500, marker='*', color='gold', zorder=3)\n",
    "# plt.annotate('GOAL', (1.0-0.18, 1.0 - 0.01), size=12)\n",
    "# plt.gca().set_aspect('equal')\n",
    "# plt.xlim(-0.1, 1.05)\n",
    "# plt.ylim(-0.0, 1.1)\n",
    "# plt.grid(alpha=0.3, linestyle='--')\n",
    "# plt.legend(loc = 'lower left', prop={'size': 10})\n",
    "# plt.xlabel('Cause Score', fontsize=12)\n",
    "# _ = plt.ylabel('Isolate Score', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
