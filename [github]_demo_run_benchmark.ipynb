{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUyOIsvigY7N"
   },
   "source": [
    "# Colab Setup\n",
    "\n",
    "This notebook requires a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flbgMtAlcLn1"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W93pPcdQgULQ"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# git clone https://github.com/explanare/ravel.git\n",
    "# git clone https://github.com/stanfordnlp/pyvene.git\n",
    "\n",
    "# pip install accelerate\n",
    "# pip install datasets\n",
    "\n",
    "# # cd pyvene\n",
    "# # git checkout d29f9591ca61753d66ba25f6cc3a4c05bab48480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zLdMswKcLn1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "RAVEL_LIB_DIR = '/share/u/can/ravel'\n",
    "RAVEL_SCRIPT_DIR = f'{RAVEL_LIB_DIR}/scripts'\n",
    "# PYVENE_LIB_DIR = '/content/pyvene'\n",
    "\n",
    "sys.path.append(RAVEL_LIB_DIR)\n",
    "sys.path.append(RAVEL_SCRIPT_DIR)\n",
    "# sys.path.append(PYVENE_LIB_DIR)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# !mkdir models\n",
    "# !mkdir data\n",
    "\n",
    "MODEL_DIR = f'{RAVEL_LIB_DIR}/models'\n",
    "DATA_DIR = f'{RAVEL_LIB_DIR}/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9-AQQvZdOJV"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "e6421f67cbeb4fd3ab7bb3264ee51df4",
      "473247a28332437d8254c896e6c57051",
      "1d4d674929df41adb6a45acc74e0863f",
      "25c03fbbdef4497397f1ab4ac36d8e30",
      "08ae93f57daa4569ba4a3e4c405ce8da",
      "70b94189e15148739dad7db7aaf8ab42",
      "0ed867cdc75540ecafa3cccda8d9855e",
      "72da486212cb432783f334f069225239",
      "ea5c4813efee43b39f2aef43ff754405",
      "545982efc183497484490d5f4f802869",
      "584b3a66e9a8494bacb5183aa92509bd",
      "c24cdc8074d6446ba36beb7ced513ad6",
      "defc4a44496140c7b4c5708b95960f96",
      "7d88ad9cef3b4995af4f8163af5b544d",
      "5f588a37541d4f13aa3b007150267804",
      "645ba29fb79748bf86a3553b970f22c2",
      "5cf0ada5f9b3495290f9fda95cdedc66",
      "005eed6540744f089e737fa1ff10079e",
      "fbba2db931df4d3d8f62380a1abcd558",
      "3479d2307a7947b5b10b2db1d780363b",
      "eac9aa0e807a4b3782a5d7235f59cbbd",
      "e0dd09f7c65848968cd77384fd731b4c",
      "446a9f40e3b94b7199e7b51717a814b7",
      "6adf85b11a484a9d9c22da15a4475d08",
      "a0c6f760286f40958d6239a4686a4559",
      "3d103360c58341e49c42a853c753e3c7",
      "458471f2712a43e5bf837c63bb6cb3f3",
      "84b20a28aaa94ba6887b0908abbdb797",
      "401b8866435b459ab499fa33db9d3120",
      "3d1321cc589a49e8afda9be94788153d",
      "0517c508b4fe4fc6b25296267753eda3",
      "1425f96afea341e28b63324ed030c42a",
      "105afa2bfd8f417f9c152369a82b5d43",
      "1b713e51221243e8b07c78e1e97dc4c4",
      "4f7b2ef7654a4ea88beb36bf39fb78e4",
      "26c0a67949044205b2607a4bd83b3527",
      "47801322a28140c4bdc4d912c77603d1",
      "d2494418df4843e1a6fafaec3045633c",
      "f336d3c4d8d142b4b3295d4a232fc8df",
      "f59f25c5edec4a3cb0296fcbbde59147",
      "57c0ac34b3c34488886ad748560a2e37",
      "756eba6c613d438fbb44fb089357824f",
      "d44c221686964c09ad24f72f87bb2733",
      "4e11a4930771477ca7dc59672e4e4f78",
      "7648cba6581c41d6807c44a1e6ec46f0",
      "9f397d441944401a9cdf31cd8cd3bb7a",
      "66d0a163396346dd8df1d3b0b7176b71",
      "bb4e97fa97b841dabdd6f48ac99b5be4",
      "fd6a5b0a0f8b45bcb9648656ce63ad4e",
      "d8ee427e21724c469ffe33de51ff4b3b",
      "13d390959219401990e19a402647d806",
      "d69cf08b63f44492954c30edb2c28439",
      "9f0846b1fb584b2589d8ae22e1b27faf",
      "d755d2bda2224d9cab2a6c1d40aac6e8",
      "6df188f81f354898a3e417b56f9f5acb",
      "6512fee1d64d432a91033ec043132684",
      "241d686011a845bea660c4870f808595",
      "12f4cb88ea9f48f4aed19ed1e44e6e29",
      "aa0e37bc24bb4efc81aa94288df79ea1",
      "4eec5229825e40bbaa19e5a10fa5bda5",
      "ebba9c999eaf42eaad6c021b34d624f7",
      "0c97a854107a48279be70b32eeefbe93",
      "82f63a30f5cd4bf1b5fab69872179dde",
      "52906e1969034d4a8c3070847d016b6f",
      "1d60c64cbe684ac9bd9c997679f7fa44",
      "6f5958342c684172a4339eaf48212a6b",
      "b83340500412489284f23181b7faa5f5",
      "481b28c71e7c4fe6a26c08b4e8b36395",
      "783f3cf86af745e7a35c97283a137322",
      "b380e2bab84f49a29405f1caa46e4672",
      "e7f9648d3ff540c6b529927e01d4d542",
      "0acc718373c0425bbf77f8c22aeb0b38",
      "863fecc9fa2148f8a80f731fc9d0ab3d",
      "8352bec67456474ca5ea922ee20a08d5",
      "2aedc3d191bc43289ade6eb2e7879f0f",
      "92eddac2de68402592536aa4d8132e88",
      "55b97afaf5a14d4f9d24c17dfb158b68"
     ]
    },
    "id": "IrrLMNHoqAiF",
    "outputId": "8e546588-a4e7-4b20-cb51-e030c70a0947"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_DIR)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, device_map='auto', cache_dir=MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16)\n",
    "model = model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "VOCAB = sorted(tokenizer.vocab, key=tokenizer.vocab.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkttf-519P79"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWqnOFocmtxI"
   },
   "source": [
    "**Download the RAVEL-TinyLlama instance (tinyllama.tgz) from [Google Drive](https://drive.google.com/file/d/1HWQADIp3aA-u7oP24OCOxGbD1m-bvzZK/view?usp=drive_link)**. The instance contains four files:\n",
    "\n",
    "* Training data for DAS & MDAS: [`tinyllama_city_train.json`](https://drive.google.com/file/d/1NWdEjkisvN_1p7fGg7qwgEAQkfJhjSrQ/view?usp=drive_link)\n",
    "\n",
    "* Entity val/test data: [`tinyllama_city_entity_test.json`](https://drive.google.com/file/d/1kXGuL_Picc4aHuTc2XpyaLBZFaji0bxC/view?usp=drive_link)\n",
    "\n",
    "* Context val/test data: [`tinyllama_city_context_test.json`](https://drive.google.com/file/d/1c8VPfg4dHDwtej31lB9DFEpI31loBBCb/view?usp=drive_link)\n",
    "\n",
    "* Intervention locations: [`tinyllama_city_prompt_to_entity_position.json`](https://drive.google.com/file/d/1DtAfnSKMRso7RAphjkp3hMjvFw7HcazB/view?usp=drive_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWYtKfM2pGMH"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# tar -xzf tinyllama.tgz -C /content/data/\n",
    "# mkdir data/base\n",
    "# tar -xvf /content/ravel/data.tgz -C data/base --strip-components=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jPOMM9W0-BL",
    "outputId": "2f5bba88-bd36-44bb-a268-9317129d122e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "instance = 'tinyllama'\n",
    "entity_type = 'city'\n",
    "INPUT_MAX_LEN = 48\n",
    "FEATURE_TYPES = datasets.Features({\"input\": datasets.Value(\"string\"), \"label\": datasets.Value(\"string\"),\n",
    "                              \"source_input\": datasets.Value(\"string\"), \"source_label\": datasets.Value(\"string\"),\n",
    "                              \"inv_label\": datasets.Value(\"string\"),\n",
    "                              'split': datasets.Value(\"string\"), 'source_split': datasets.Value(\"string\"),\n",
    "                              'entity': datasets.Value(\"string\"), 'source_entity': datasets.Value(\"string\")})\n",
    "\n",
    "\n",
    "# Load training dataset.\n",
    "split_to_raw_example = json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_train.json'), 'r'))\n",
    "# Load validation + test dataset.\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_context_test.json'), 'r')))\n",
    "split_to_raw_example.update(json.load(open(os.path.join(DATA_DIR, f'{instance}/{instance}_{entity_type}_entity_test.json'), 'r')))\n",
    "# Prepend an extra token to avoid tokenization changes for Llama tokenizer.\n",
    "# Each sequence will start with <s> _ 0\n",
    "SOS_PAD = '0'\n",
    "NUM_SOS_TOKENS = 3\n",
    "for split in split_to_raw_example:\n",
    "  for i in range(len(split_to_raw_example[split])):\n",
    "    split_to_raw_example[split][i]['inv_label'] = SOS_PAD + split_to_raw_example[split][i]['inv_label']\n",
    "    split_to_raw_example[split][i]['label'] = SOS_PAD + split_to_raw_example[split][i]['label']\n",
    "\n",
    "\n",
    "# Load attributes (tasks) to prompt mapping.\n",
    "ALL_ATTR_TO_PROMPTS = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_attribute_to_prompts.json')))\n",
    "\n",
    "# Load prompt to intervention location mapping.\n",
    "split_to_entity_pos = json.load(open(os.path.join(DATA_DIR, instance, f'{instance}_{entity_type}_prompt_to_entity_position.json')))\n",
    "SPLIT_TO_INV_LOCATIONS = {\n",
    "    f'{task}{split}': {'max_input_length': INPUT_MAX_LEN,\n",
    "                       'inv_position': [INPUT_MAX_LEN + pos]}\n",
    "    for task, pos in split_to_entity_pos.items()\n",
    "    for split in ('-train', '-test', '-val', '')\n",
    "}\n",
    "assert(min([min(v['inv_position']) for v in SPLIT_TO_INV_LOCATIONS.values()]) > 0)\n",
    "\n",
    "\n",
    "# Preprocess the dataset.\n",
    "def filter_inv_example(example):\n",
    "  return (example['label'] != example['inv_label'] and\n",
    "          example['source_split'] in SPLIT_TO_INV_LOCATIONS and\n",
    "          example['split'] in SPLIT_TO_INV_LOCATIONS)\n",
    "\n",
    "for split in split_to_raw_example:\n",
    "  random.shuffle(split_to_raw_example[split])\n",
    "  split_to_raw_example[split] = list(filter(filter_inv_example, split_to_raw_example[split]))\n",
    "  if len(split_to_raw_example[split]) == 0:\n",
    "    print('Empty split: \"%s\"' % split)\n",
    "# Remove empty splits.\n",
    "split_to_raw_example = {k: v for k, v in split_to_raw_example.items() if len(v) > 0}\n",
    "print(f\"#Training examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-train')]))}, \"\n",
    "      f\"#Validation examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-val')]))}, \"\n",
    "      f\"#Test examples={sum(map(len, [v for k, v in split_to_raw_example.items() if k.endswith('-test')]))}\")\n",
    "split_to_dataset = {split: Dataset.from_list(\n",
    "    split_to_raw_example[split], features=FEATURE_TYPES)\n",
    "                    for split in split_to_raw_example}\n",
    "\n",
    "# #Training examples=116728, #Validation examples=20516, #Test examples=22497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_ATTR_TO_PROMPTS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKpdF24zzJN-"
   },
   "source": [
    "# Sparse Autoencoder (SAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZuIUk2DZaO"
   },
   "source": [
    "## Training\n",
    "\n",
    "We will train a sparse autoencoder on entity representations extracted offline.\n",
    "\n",
    "* Download entity representations extracted from the Wikipedia dataset [here](https://drive.google.com/file/d/1hZ-Nv3ehf0Ok4ic3ybe-DATEh-HRjYkt/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmtffTWJkrJY",
    "outputId": "7f243b42-e04b-4aa0-9123-9ac0cd055977"
   },
   "outputs": [],
   "source": [
    "from src.utils.dataset_utils import HDF5Dataset\n",
    "\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, \"wikipedia_20220301_city_tinyllama_layer14_features_200k.hdf5\")\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "train_dataset = HDF5Dataset(file_path, sample_range=None)\n",
    "val_dataset = HDF5Dataset(file_path, sample_range=list(range(80, 128)))\n",
    "wiki_train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=True)\n",
    "wiki_val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIZ91X8nxFRa",
    "outputId": "722da858-159d-4782-b282-233e00da3885"
   },
   "outputs": [],
   "source": [
    "task_name = 'tinyllama-layer14-dim8192-reg0.5-ep1-sae-city_wikipedia_200k' #@param\n",
    "task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzRVRB6DDZYe",
    "outputId": "f50c6403-49d9-4c75-b0f3-eb828c3948f7"
   },
   "outputs": [],
   "source": [
    "from train_sae import train_sae\n",
    "import regex as re\n",
    "\n",
    "config = {\n",
    "    'task_name': task_name,\n",
    "    'reg_coeff': float(re.search('reg([\\d.]+)', task_name).group(1)),\n",
    "    'input_dim': model.config.hidden_size,\n",
    "    'latent_dim': int(re.search('dim(\\d+)', task_name).group(1)),\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'end_learning_rate_ratio': 0.5,\n",
    "    'num_epochs': int(re.search('ep(\\d+)', task_name).group(1)),\n",
    "    'model_dir': MODEL_DIR,\n",
    "    'log_dir': os.path.join(MODEL_DIR, 'logs', task_name),\n",
    "}\n",
    "\n",
    "# Training metrics are logged to the Tensorboard at http://localhost:6006/.\n",
    "autoencoder = train_sae(config, wiki_train_dataloader, wiki_val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBClAbyY0Tm0"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6E0eudgO2F"
   },
   "outputs": [],
   "source": [
    "autoencoder_run_name = 'tinyllama-layer14-dim8192-reg0.5-ep1-sae-city_wikipedia_200k.pt'\n",
    "autoencoder = torch.load(os.path.join(MODEL_DIR, autoencoder_run_name)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OeMovewbVdp",
    "outputId": "8e00026e-b2a6-4524-fa9e-94dce75065ed"
   },
   "outputs": [],
   "source": [
    "# Load the RAVEL dataset.\n",
    "import json\n",
    "\n",
    "from src.utils.dataset_utils import load_entity_representation_with_label\n",
    "\n",
    "\n",
    "entity_type = 'city'\n",
    "layer = 14\n",
    "print(entity_type, layer)\n",
    "\n",
    "model_type = 'tinyllama'\n",
    "splits = ['train', 'val_entity', 'val_context']\n",
    "feature_hdf5_path = os.path.join(DATA_DIR, f'ravel_{entity_type}_{model_type}_layer{layer}_representation.hdf5')\n",
    "entity_attr_to_label = json.load(open(os.path.join(DATA_DIR, 'base', f'ravel_{entity_type}_entity_attributes.json')))\n",
    "X, Y, sorted_unique_label = load_entity_representation_with_label(feature_hdf5_path, entity_attr_to_label, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPF6FR8zvwvJ"
   },
   "outputs": [],
   "source": [
    "# Run feature selection.\n",
    "import numpy as np\n",
    "\n",
    "from src.methods.select_features import select_features_with_classifier\n",
    "\n",
    "intervention_dim_to_eval = [\n",
    "    ('reconstruction', None),\n",
    "    ('dim%d' % autoencoder.encoder[0].out_features, range(autoencoder.encoder[0].out_features))]\n",
    "\n",
    "attr = 'Country'\n",
    "coeff_to_kept_dims = select_features_with_classifier(\n",
    "    autoencoder.encode, torch.from_numpy(X[attr]['train']).to(device), Y[attr]['train'])\n",
    "for kept_dim in coeff_to_kept_dims.values():\n",
    "  intervention_dim_to_eval.append(('dim%d' % len(kept_dim), kept_dim))\n",
    "# Random baselines.\n",
    "for i in [64, 512]:\n",
    "    kept_dim = np.random.permutation(autoencoder.encoder[0].out_features)[:i]\n",
    "    intervention_dim_to_eval.append(('random_dim%d' % len(kept_dim), kept_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0npE3xlExIz"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF6JzDbPzIu8",
    "outputId": "7abbaeb2-c41a-46db-b338-1b2aa43d00c1"
   },
   "outputs": [],
   "source": [
    "# Run eval\n",
    "import re\n",
    "\n",
    "from src.utils.intervention_utils import load_intervenable_with_autoencoder, eval_with_interventions, remove_all_forward_hooks\n",
    "from src.utils.metric_utils import compute_metrics\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')\n",
    "                         }\n",
    "\n",
    "target_task = 'Country'\n",
    "max_new_tokens = 3\n",
    "layer = int(re.search(r'layer(\\d+)', autoencoder_run_name).group(1))\n",
    "print(f'Layer={layer}')\n",
    "\n",
    "for inv_name, inv_dims in intervention_dim_to_eval:\n",
    "  if inv_name == 'reconstruction':\n",
    "    continue\n",
    "  intervenable = load_intervenable_with_autoencoder(model, autoencoder, inv_dims, layer)\n",
    "  intervenable.set_device(\"cuda\")\n",
    "  intervenable.disable_model_gradients()\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "      intervenable, eval_split_to_dataset, SPLIT_TO_INV_LOCATIONS, tokenizer,\n",
    "      max_new_tokens=max_new_tokens,\n",
    "      compute_metrics_fn=compute_metrics,\n",
    "      eval_batch_size=128,\n",
    "      debug_print=False)\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{autoencoder_run_name.split(\".pt\")[0]}_{inv_name}_{max_new_tokens}tok_{target_task}.json'), 'w'))\n",
    "  remove_all_forward_hooks(intervenable)\n",
    "  del intervenable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = f'tinyllama-layer14-dim8192-reg0.5-ep1-sae-city_wikipedia_200k_dim170_3tok_Country.json'\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, result_filename), 'r') as f:\n",
    "    result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuMUHhoBohzd"
   },
   "source": [
    "# Distributed Alignment Search (DAS/MDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onC3-T2h0ulg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DGCvKEQxYsU",
    "outputId": "c9913675-0723-45ca-f0a8-2cbbd37368cb"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "from methods.distributed_alignment_search import LowRankRotatedSpaceIntervention\n",
    "from methods.differential_binary_masking import DifferentialBinaryMasking\n",
    "import pyvene as pv\n",
    "from tqdm import tqdm, trange\n",
    "from train_intervention import train_intervention\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from utils.dataset_utils import get_multitask_dataloader\n",
    "from utils.intervention_utils import train_intervention_step, eval_with_interventions, get_intervention_config, remove_all_forward_hooks, remove_invalid_token_id\n",
    "from utils.metric_utils import compute_metrics, compute_cross_entropy_loss\n",
    "\n",
    "\n",
    "def get_short_model_name(model):\n",
    "  name_match = re.search('(llama-2-\\d+b|tinyllama|pythia-[\\d.]+b)', model.name_or_path.lower())\n",
    "  if name_match:\n",
    "    return name_match.group(1)\n",
    "  else:\n",
    "    return model.name_or_path.lower().split('-')[0]\n",
    "\n",
    "\n",
    "def run_exp(config):\n",
    "  inv_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_source' in l])\n",
    "  control_tasks = '+'.join([''.join(re.findall(r'[A-Za-z]+', t)) for t, l in config['training_tasks'].items() if 'match_base' in l])\n",
    "  task_compressed = ((inv_tasks + '_ex_' + control_tasks) if control_tasks else inv_tasks)\n",
    "  method_name = 'multitask_method' if len(config['training_tasks']) > 1 else 'baseline_method'\n",
    "  if config['intervenable_config']['intervenable_interventions_type'] == LowRankRotatedSpaceIntervention:\n",
    "    method_name = method_name.replace('method', 'daslora')\n",
    "  elif config['intervenable_config']['intervenable_interventions_type'] == DifferentialBinaryMasking:\n",
    "    if config['regularization_coefficient'] > 1e-6:\n",
    "      method_name = method_name.replace('method', 'mask_l1')\n",
    "    else:\n",
    "      method_name = method_name.replace('method', 'mask')\n",
    "  split_to_inv_locations = config['split_to_inv_locations']\n",
    "  input_len = list(split_to_inv_locations.values())[0]['max_input_length']\n",
    "  inv_pos = min([x['inv_position'][0] for x in split_to_inv_locations.values()])\n",
    "  inv_loc_name = 'len%d_pos%s' % (input_len, 'e' if inv_pos != input_len - 1 else 'f')\n",
    "  training_data_percentage = int(config['max_train_percentage'] * 100)\n",
    "  suffix = f\"_cause{config['cause_task_sample_size']}\"\n",
    "  if any([v == 'match_base' for t, v in config['training_tasks'].items()]):\n",
    "    suffix += f'_iso{config[\"iso_task_sample_size\"]}'\n",
    "  layer = config['intervenable_config']['intervenable_layer']\n",
    "  run_name = (f\"{get_short_model_name(model)}-layer{layer}\"\n",
    "              f\"-dim{config['intervention_dimension']}\"\n",
    "              f\"-{method_name}_{config['max_output_tokens']}tok_\"\n",
    "              f\"{task_compressed}_{inv_loc_name}_ep{config['training_epoch']}{suffix}\")\n",
    "  config['run_name_prefix'] = run_name.rsplit('_ep', 1)[0]\n",
    "  print(run_name)\n",
    "  intervenable, intervenable_config = train_intervention(config, model, tokenizer, split_to_dataset)\n",
    "  # Save model.\n",
    "  torch.save({k: v[0].rotate_layer.weight for k, v in intervenable.interventions.items()},\n",
    "             os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  print('Model saved to %s' % os.path.join(MODEL_DIR, f'{run_name}.pt'))\n",
    "  # Eval.\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "      intervenable, eval_split_to_dataset, split_to_inv_locations, tokenizer,\n",
    "      compute_metrics_fn=compute_metrics,\n",
    "      max_new_tokens=config['max_output_tokens'],\n",
    "      eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  # Logging.\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))\n",
    "  print('Saved to %s' % os.path.join(MODEL_DIR, f'{run_name}_evalall.json'))\n",
    "  remove_all_forward_hooks(intervenable)\n",
    "  return intervenable\n",
    "\n",
    "\n",
    "attrs = list(ALL_ATTR_TO_PROMPTS)\n",
    "target_attr = 'Country'\n",
    "\n",
    "# Train on disentangling Country attribute only.\n",
    "training_tasks_list = [\n",
    "  {t: 'match_source'} for t in attrs if t == target_attr\n",
    "] + [\n",
    "    {t: 'match_source' if t == target_t else 'match_base' for t in attrs}\n",
    "    for target_t in attrs if target_t == target_attr\n",
    "]\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test') or k.endswith('-val')}\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 128\n",
    "\n",
    "lr = 1e-4\n",
    "for inv_layer in [14]:\n",
    "  for inv_dim in [64]:\n",
    "    for training_tasks in training_tasks_list:\n",
    "      for cause_task_sample_size in [20000]:\n",
    "        config = {\n",
    "            'regularization_coefficient': 0,\n",
    "            'intervention_dimension': inv_dim,\n",
    "            'max_output_tokens': 3,\n",
    "            'intervenable_config': {\n",
    "              'intervenable_layer': inv_layer,\n",
    "              'intervenable_representation_type': 'block_output',\n",
    "              'intervenable_unit': 'pos',\n",
    "              'max_number_of_units': 1,\n",
    "              'intervenable_interventions_type': LowRankRotatedSpaceIntervention,\n",
    "            },\n",
    "            'training_tasks': training_tasks,\n",
    "            'training_epoch': 3,\n",
    "            'split_to_inv_locations': SPLIT_TO_INV_LOCATIONS,\n",
    "            'max_train_percentage': 1.0 if len(training_tasks) <= 3 else 1.0,\n",
    "            'init_lr': lr,\n",
    "            'cause_task_sample_size': cause_task_sample_size,\n",
    "            'iso_task_sample_size': 4000,\n",
    "            'training_batch_size': TRAINING_BATCH_SIZE,\n",
    "            'task_to_prompts': ALL_ATTR_TO_PROMPTS,\n",
    "            'log_dir': os.path.join(MODEL_DIR, 'logs'),\n",
    "        }\n",
    "        intervenable = run_exp(config)\n",
    "\n",
    "\n",
    "# Training each method will take about 3.5 hrs on the hosted T4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glatG86QlJ4q"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgTg14xMlJCO"
   },
   "outputs": [],
   "source": [
    "# The training script above has already included the evaluation part.\n",
    "# Below is a standalone evaluation script in case you want to rerun evaluation.\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import pyvene as pv\n",
    "from utils.intervention_utils import load_intervenable, load_intervenable_with_pca, eval_with_interventions\n",
    "from utils.metric_utils import compute_metrics\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    'tinyllama-layer14-dim64-multitask_daslora_3tok_Country_ex_Continent+Latitude+Longitude+Language+Timezone_len48_pose_ep3_cause20000_iso4000.pt',\n",
    "    'tinyllama-layer14-dim64-baseline_daslora_3tok_Country_len48_pose_ep3_cause20000.pt',\n",
    " ]\n",
    "\n",
    "eval_split_to_dataset = {k: v for k, v in split_to_dataset.items()\n",
    "                         if k.endswith('-test')\n",
    "                         }\n",
    "RUN_TO_EVAL_METRICS = {}\n",
    "for i, run_name in enumerate(model_paths):\n",
    "  print(run_name)\n",
    "  layer = int(re.search('layer(\\d+)[_\\-]', run_name).group(1))\n",
    "  run_name, ext = run_name.rsplit('.', 1)\n",
    "  if 'pca' in run_name:\n",
    "    intervenable = load_intervenable_with_pca(model, run_name + '.' + ext)\n",
    "  elif 'causal_abstraction' in run_name:\n",
    "    intervenable = load_causal_abstraction_intervenable(model, run_name)\n",
    "  else:\n",
    "    intervenable = load_intervenable(model, os.path.join(MODEL_DIR, run_name + '.' + ext))\n",
    "  split_to_eval_metrics = eval_with_interventions(\n",
    "      intervenable, eval_split_to_dataset, SPLIT_TO_INV_LOCATIONS if layer < 24 else SPLIT_TO_INV_LOCATIONS_LAST_TOK,\n",
    "      tokenizer, compute_metrics_fn=compute_metrics, max_new_tokens=3, debug_print=False)\n",
    "  json.dump(split_to_eval_metrics, open(os.path.join(MODEL_DIR, f'{run_name}_evalall.json'), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-_gzLeA1Qc7"
   },
   "source": [
    "# Compare Methods with Disentangle Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "_0GSxcY1esIh",
    "outputId": "a140cf5d-71f5-4457-834c-1c51a83b7218"
   },
   "outputs": [],
   "source": [
    "#@markdown TODO: Add results parsing & plotting code.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "\n",
    "colors = [matplotlib.colors.to_hex(c) for c in plt.cm.tab20.colors]\n",
    "\n",
    "name_to_color = {\n",
    "    'SAE_RAND': 'gray',\n",
    "    'PCA': colors[6],\n",
    "    'SAE': colors[2],\n",
    "    'RLAP': colors[4],\n",
    "    'DBM': colors[1],\n",
    "    'MDBM': colors[0],\n",
    "    'DAS': colors[9],\n",
    "    'MDAS': colors[8],\n",
    "}\n",
    "\n",
    "name_to_marker = {\n",
    "    'SAE_RAND': 'o--',\n",
    "    'PCA': 'o--',\n",
    "    'SAE': 'o--',\n",
    "    'RLAP': '^--',\n",
    "    'DBM': 's--',\n",
    "    'MDBM': 's--',\n",
    "    'DAS': 's--',\n",
    "    'MDAS': 's--',\n",
    "}\n",
    "\n",
    "for n, x in method_to_data.items():\n",
    "  sorted_dim = sorted(x, key=lambda i: float(i[:-1]))\n",
    "  p = plt.plot([x[k][2] for k in sorted_dim],\n",
    "               [x[k][1] for k in sorted_dim], name_to_marker[n], label=n, markersize=10,\n",
    "               c=name_to_color[n])\n",
    "  for k in sorted(x, key=lambda s: x[s][0], reverse=True):\n",
    "    c = p[-1].get_color()\n",
    "    offset = (0, 0.05)\n",
    "    # Shift text boxes to avoid overlaps.\n",
    "    if n == 'SAE' and k == '3.8%':\n",
    "      offset = (0.05, -0.07)\n",
    "    plt.annotate(k, (x[k][2] - offset[0], x[k][1] + offset[1]), size=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.15', fc=c, ec='white', alpha=0.5))\n",
    "plt.scatter(1, 1, s=500, marker='*', color='gold', zorder=3)\n",
    "plt.annotate('GOAL', (1.0-0.18, 1.0 - 0.01), size=12)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlim(-0.1, 1.05)\n",
    "plt.ylim(-0.0, 1.1)\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.legend(loc = 'lower left', prop={'size': 10})\n",
    "plt.xlabel('Cause Score', fontsize=12)\n",
    "_ = plt.ylabel('Isolate Score', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
